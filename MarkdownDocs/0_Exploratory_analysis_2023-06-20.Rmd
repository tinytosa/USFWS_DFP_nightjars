---
title: "Nightjar exploratory analysis"
author: "Marie I. Tosa"
date: "`r Sys.Date()`"
output: pdf_document
---

## document setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load packages

#for playing with data
require(tidyr) #for separate
require(readxl) #for read_excel
require(openxlsx) #for read.xlsx
require(dplyr) #for bind_rows
require(plyr)

#for GIS
require(sf)
require(raster)
require(units)

#for plotting
require(ggplot2)


```

## GIS layers

Look at states within the Atlantic Flyway and the Bird Conservation Regions (BCR)

```{r}

AF_states <- st_read(dsn="../../GIS", layer="AF_states") 
# BCR <- read_sf(dsn="../../GIS", layer="BCR_AF_clip") #don't use this one. not up to date. This one was from Tony Roberts
#these are in NAD 83, longlat

BCR.all <- read_sf(dsn="../../GIS/BCR_Terrestrial", layer="BCR_Terrestrial_master") #https://nabci-us.org/resources/bird-conservation-regions-map/
#http://www.birdscanada.org/research/gislab/index.jsp?targetpg=bcr

AF_states.df <- AF_states
st_geometry(AF_states.df) <- NULL
AF_states.df$STATE_NAME <- toupper(AF_states.df$STATE_NAME)

BCR.AF <- BCR.all[BCR.all$PROVINCE_S %in% toupper(AF_states$STATE_NAME),]
BCR.AF <- merge(BCR.AF, AF_states.df[,c("STATE_NAME","ST")], 
                by.x="PROVINCE_S", by.y="STATE_NAME", all.x=T)
BCR.AF <- st_collection_extract(BCR.AF, "POLYGON")
BCR.AF$AreaSqMi <- set_units(st_area(BCR.AF), "mi^2")

# st_write(BCR.AF, "../../GIS/BCR-state.shp", append=F) #to save

BCR <- read_sf(dsn="../../GIS", layer="BCR-state") #Area in m^2

plot(AF_states)
plot(BCR)

unique(BCR$BCR)
#7 BCRs
#14 13 30 28 29 27 31

AF_states$ST
#18 states
# "CT" "DE" "DC" "FL" "GA" "ME" "MD" "MA" "NH" "NJ" "NY" "NC" "PA" "RI"
# "SC" "VT" "VA" "WV"

AF_states$AreaSqMi <- set_units(st_area(AF_states), "mi^2")

#don't need to do this with the new BCR shapefile from 
#create polygon shapefile with each state cut by BCR
# BCR.ST <- st_intersection(AF_states, BCR)
# BCR.ST$BCR.ST <- paste(BCR.ST$ST, BCR.ST$BCRNumber, sep=".")
# BCR.ST <- st_collection_extract(BCR.ST, "POLYGON") #otherwise has polygons, multipolygons, geometrycollection, etc.
# 
# sf_use_s2(FALSE)
# BCR.ST$AreaBS <- set_units(st_area(BCR.ST), "mi^2")

# st_write(BCR.ST, "../../GIS/BCR-state.shp", append=F) #to save
```

## Raw data from Nightjar Survey Network
format data so easier to work with later

```{r, echo=T}
#raw files
nsn <- read.csv(file="../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Routes.csv")
#duplicate routeid when there are stops 1-11
  # nsn.all <- nsn
  # nsn <- nsn[nsn$Stop == 1 | nsn$Routeid == 981234,] #subset for first stop in each route, no Stop 1 for 981234

rt.id <- read.csv("../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Route ID.csv")

nsn.sp <- read.csv("../Data_raw/NSN_sp_key.csv")

nsn.time <- read.csv("../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Time and Miles.csv")
  nsn.time <- separate(nsn.time, col="start_time", remove=F,
                       into=c("start_date","start_hour"), sep=" ")
  nsn.time <- separate(nsn.time, col="end_time", remove=F,
                       into=c("end_date","end_hour"), sep=" ")
  nsn.time <- separate(nsn.time, col="start_date",
                       into=c("start_year","start_month","start_day"),
                       sep="-", remove=F)
  
  unique(nsn.time[nsn.time$route_id < 10000,]$route_id)
  # unique(nsn.time[nsn.time$route_id < 100000,]$route_id)
  
  #correct these route_ids
  nsn.time[nsn.time$route_id == 6101,]$route_id <- 60101
  nsn.time[nsn.time$route_id == 6051,]$route_id <- 60051

nsn.obs <- read.csv("../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Point Count Observations.csv") #39,658 observations
  #are there multiple entries per nsn.sampling?
  length(unique(nsn.obs[duplicated(nsn.obs[,c("survey_stop_id","species_id")]),]$survey_stop_id)) #hmmmmm...9640 sampling occasions with multiple observations!
  #25% of the observations 
  
  # nsn.sampling[nsn.sampling$id %in% 363,] #to get survey_id
  # nsn.sampling[nsn.sampling$survey_id %in% 37,] #to get id.sampling to see if there are other observations within the route
  # nsn.obs[nsn.obs$survey_stop_id %in% 361:370,]
  
  # master.nsn[master.nsn$id.sampling %in% 10941,]
  # nsn.sampling[nsn.sampling$survey_id %in% 1095,]
  # nsn.all[nsn.all$Routeid %in% 890027,]
  
  #consolidate multiple rows with the same survey_stop_id
  nrow(nsn.obs[duplicated(nsn.obs[,c("survey_stop_id","species_id")]),]) #19,234 observations for sampling occasion that already has an observation
  #convert t/f to 1/0
  nsn.obs[,grep(names(nsn.obs), pattern="minute", value=T)][nsn.obs[,grep(names(nsn.obs), pattern="minute", value=T)] == "t"] <- 1
  nsn.obs[,grep(names(nsn.obs), pattern="minute", value=T)][nsn.obs[,grep(names(nsn.obs), pattern="minute", value=T)] == "f"] <- 0
  
  nsn.obs <- ddply(nsn.obs, .(survey_stop_id, species_id), summarize, 
        min.1=sum(as.numeric(heard_minute_1)),
        min.2=sum(as.numeric(heard_minute_2)),
        min.3=sum(as.numeric(heard_minute_3)),
        min.4=sum(as.numeric(heard_minute_4)),
        min.5=sum(as.numeric(heard_minute_5)),
        min.6=sum(as.numeric(heard_minute_6)))
  nrow(nsn.obs) # 20,424 rows

nsn.sampling <- read.csv("../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Point Count Sampling.csv")
  nsn.sampling <- nsn.sampling[order(nsn.sampling$id),]
  nsn.sampling$stop_mit <- rep(1:10, times=nrow(nsn.sampling)/10)

nsn.old <- read.csv("../Data_raw/Copy of NSN Data/NSN Data (March 28 2023) - Old Routes.csv")

#rt.id id numbers to fix
rt.id.correct <- read.csv("../Data_raw/NSN_routeid_corrections.csv",
                          colClasses = "character")
```

##Fix unmatched route id so matches routes (and location information)
```{r, echo=T}
#########
#routes

#convert 5 digit route id to 6 digit
nsn$Routeid.char <- sprintf("%06i", nsn$Routeid)
nsn$st_code <- substr(nsn$Routeid.char, 0, 2)
nsn$rt_code <- substr(nsn$Routeid.char, 3, 6)

# write.table(unique(nsn$st_code), file="../Data_raw/NSN_routeid_codes.txt", sep=",", row.names=F)
#determine states manually using graphing of points below

summary(nsn)
nsn.sf <- st_as_sf(nsn, coords=c("Latitude","Longitude"), crs=4269)
plot(nsn.sf)

# unique(nsn$Routeid)

# plot(nsn.sf[grep(nsn.sf$Routeid, pattern="^02"),])

base.plot <- ggplot() + geom_sf(data=nsn.sf, col="white") +
  geom_sf(data=AF_states) + theme_bw() +
  theme(panel.grid=element_blank())
# base.plot + geom_sf(data=nsn.sf[nsn.sf$st_code == "98",])

#read csv file back in
nsn.state.codes <- read.csv("../Data_raw/NSN_routeid_codes.csv",
                            colClasses = "character")

nsn <- merge(nsn, nsn.state.codes, by.x="st_code",
             by.y="routeid_code", all.x=T)

#########
#route id
summary(rt.id)
nrow(rt.id)
table(rt.id$state)

rt.id$route_id.char <- sprintf("%06i", rt.id$id)
rt.id$st_code <- substr(rt.id$route_id.char, 0,2)
rt.id$rt_code <- substr(rt.id$route_id.char, 3,6)

# verify <- unique(rt.id[,c("state","st_code")])
# write.table(verify[duplicated(verify$state),], sep=",", file = "../Data_raw/nsn_routeid_tofix.txt", row.names=F)

#read in corrected file and correct id numbers so they match nsn routes.csv
rt.id <- merge(rt.id.correct[,c("original","corrected")], rt.id,
               by.x="original", by.y="id", all.y=T)
rt.id[is.na(rt.id$corrected),]$corrected <- rt.id[is.na(rt.id$corrected),]$original

rt.id$route_id.char <- sprintf("%06i", as.integer(rt.id$corrected))
rt.id$st_code <- substr(rt.id$route_id.char, 0,2)
rt.id$rt_code <- substr(rt.id$route_id.char, 3,6)

unique(rt.id[,c("state","st_code")])

#NY, ME, MA, WI, NH, DE, VT, and AZ all have 00 state codes

######
#other corrections
rt.id[rt.id$st_code == 36 & rt.id$state == "ID",]$state <- "IA" #is this correct???

#check for duplicated rt.id$corrected
rt.id[duplicated(rt.id$corrected),]
nrow(rt.id)
rt.id <- rt.id[!duplicated(rt.id$corrected),]
nrow(rt.id) #removed 68 rows
```

##Try merging NSN data together to create master data frame

rt.id = 4325 rows
nsn = 3418 rows now 6002 rows?

don't do this anymore. follow order of NSN network folks (next code chunk)

```{r, echo=T}
# # master <- merge(nsn, rt.id, all=T, by.x="Routeid", by.y="id",
# #                 suffixes=c(".rt","rtid"))
# #don't do this with original rt.id data frame since unmatched routeIDs
# 
# #this rt.id has location information already
# master <- merge(rt.id, nsn, all=T,
#                 by.x="route_id.char", by.y="Routeid.char",
#                 suffixes=c("",".rt"))
# #if has lat/long, then good, if not need to fix, add to NSN_rotueid_corrections.csv
# 
#   # master[is.na(master$Latitude) & master$state %in% AF_states$ST,]
#   #data from Atlantic Flyway that doesn't have location information
#   
#   master[duplicated(master$corrected),]$route_id.char
#   #"259249" "550037" "880001" #all of these have NA original & correct
#   # master[master$route_id.char %in% c("259249"),]
#   
#   master[master$Stop %in% 11,]
#   #Stop == 11 in AZ and MN?
# 
# master <- merge(master, nsn.time, by.x="corrected", by.y="route_id", 
#                 all=T, suffixes=c("",".time"))
# nrow(master) #7374 rows
# master <- merge(master, nsn.sampling, by.x="id",
#                 by.y="survey_id", all=T, suffixes=c("",".sampling"))
# nrow(master) #49418 rows
# master <- merge(master, nsn.obs, by.x="id.sampling",
#                 by.y="survey_stop_id", all=T, suffixes=c("",".obs"))
# nrow(master)
# master <- merge(master, nsn.sp, all.x=T, by.x="species_id", by.y="id",
#                 suffixes=c("",".sp"))
# nrow(master)
# 
# # unique(master[is.na(master$id),]$route_id.char) #route sampling info and time/miles info, but no route info, no route id
# #are these duplicated entries?
# #221
# 
# write.table(master, file="../Data_processed/master_nsn_2023-06-21.txt", sep=",", row.names=F)
```

## merge in order that nsn people do it
- need to add nsn.time and nsn.sampling together so can merge with route_id and stop_id
- multiple rows of observations for a single survey means multiple birds were observed

```{r, echo=T}
#1. add nsn.obs to nsn.sampling

  #check how many rows in nsn.obs don't have corresponding nsn.sampling
  nsn.obs[!nsn.obs$survey_stop_id %in% nsn.sampling$id,] #none!

  #check how many nsn.sampling don't have corresponding nsn.sampling
  # nsn.sampling[!nsn.sampling$id %in% nsn.obs$survey_stop_id,] #28,701 rows!!! lots of missing observation data
  nrow(nsn.sampling[!nsn.sampling$id %in% nsn.obs$survey_stop_id,])

master <- merge(nsn.obs, nsn.sampling, by.x="survey_stop_id", by.y="id", all=T, suffixes=c(".obs",".sampling")) #49,125 rows 8/127/2023; 68,359 rows (nrow(nsn.obs) + 28,701)

#2. add nsn.time to master
master <- merge(master, nsn.time, by.x="survey_id", by.y="id", all=T, suffixes=c("",".time")) #49,125 rows

#3. add nsn to master
  nrow(nsn[!(nsn$Routeid %in% master$route_id),]) #2663 rows (roughly the number of rows added with the merge aka routes without observations)
master <- merge(master, nsn, by.x=c("route_id","stop_mit"), by.y=c("Routeid","Stop"), all=T, suffixes=c("",".route")) #51,790 rows 8/17/2023; 71,024 rows

#4. add rt.id to master
  nrow(rt.id[!(rt.id$corrected %in% master$route_id),]) #420 rows not in master already
master <- merge(master, rt.id, by.x="route_id", by.y="corrected", all=T, suffixes=c("",".rtid")) #52,210 rows 8/17/2023; 71,444 rows

#5 add nsn.sp to master
master <- merge(master, nsn.sp[,c("id","common_name","scientific_name","code")], by.x="species_id", by.y="id", all.x=T) #52,210 rows

# master[,c("state","state.rtid")]
master$state <- master$state.rtid

#do this once
# write.table(master, file="../Data_processed/master_nsn_2023-07-24.txt", sep=",", row.names=F)

```

## Raw data from non-Nightjar Survey Network
```{r, echo=T}

#read in raw data
non.nsn <- read.csv("../Data_raw/CombinedData/Allobs_noNJN.csv")
non.nsn.sites <- read.csv("../Data_raw/CombinedData/Allroute_info.csv")

##########
#read in CT data
##########

# ct.1 <- read_excel("../Data_raw/CT Whippoorwill Data/WPWL2010.xls",
#                    sheet="DataSummary4")
#can also read in DataAnalysisInterval2 for counts by interval

ct.file.list <- dir("../Data_raw/CT Whippoorwill Data",
                    pattern=".xls", full.names = T)
ct.data.all <- data.frame()
ct.obs.all <- data.frame()
for(f in ct.file.list)
{
  print(f)
  ct.data <- read_excel(f, sheet="DataSummary4")
  ct.data$file <- f
  ct.data$year <- as.numeric(gsub(gsub(f,
                  pattern="../Data_raw/CT Whippoorwill Data/WPWL",
                  replacement=""), pattern=".xls", replacement=""))
  ct.data.all <- rbind(ct.data.all, ct.data)
  
  ct.obs <- read_excel(f, sheet="DataAnalysisInterval2")
  ct.obs$file <- f
  ct.obs$year <- as.numeric(gsub(gsub(f,
                  pattern="../Data_raw/CT Whippoorwill Data/WPWL",
                  replacement=""), pattern=".xls", replacement=""))
  ct.obs.all <- rbind(ct.obs.all, ct.obs)

}
# write.table(file="../Data_raw/CT Whippoorwill Data/master_CT_WPWL.txt", 
#             ct.data.all, sep=",", row.names=F)

# write.table(file="../Data_raw/CT Whippoorwill Data/master_obs_CT_WPWL.txt",
#             ct.obs.all, sep=",", row.names=F)

########
# read in Massachusetts data
########
#this excel document is a mess!
######before running this script, make sure cells are merged for each route for columns "SITE" and "DATE", fix errors in times and longitude#######
#no minute by minute detection history, only number detected per stop
#NA rows in between each route
#Notes contain start time and end time of survey, survey conditions, plus extra information
#2013 data coordinates in DMS (epsg: 9107 ??), all other data in decimal-degrees (epsg: 4326 ??)
#EPSG: 4326 is WGS84, decimal degrees
#EPSG: 4269 is NAD83, decimal degrees
#EPSG: 9107 is degree, minute, second? can't input in the format, so need to convert to decimal degrees manually

#added new column in 2022 called EO_num?

#approach
#step 1: route/stop info: route name, stop num, lat/long
#step 2: survey info: route name, date, observer, notes (split out time, sky conditions, noise)
#step 3: obs info: stop num, EWPW
#step 4: combine all 3 data frames for one master massachusetts data frame (save this)

#info not used: direction, distance

ma <- NULL
ma.rt <- NULL
ma.survey <- NULL
ma.obs <- NULL

for(y in 2013:2022)
{
  print(y)
  #get route information
  ma.y.fill <- read.xlsx("../Data_raw/CombinedData/NightjarSurvey_MA_DataResults_2013-2022.xlsx", sheet=as.character(y), fillMergedCells=T)
  # ma.y.nofill <- read_excel("../Data_raw/CombinedData/NightjarSurvey_MA_DataResults_2013-2022.xlsx", sheet=as.character(y)) #this isn't very useful because all cells except top left cell of merged cells get turned into NAs

  names(ma.y.fill) <- toupper(names(ma.y.fill))
  ma.y.fill$DATE <- as.numeric(trimws(ma.y.fill$DATE))
  #convert excel date (numeric) to character
  ma.y.fill$DATE <- format(as.Date(ma.y.fill$DATE, origin = "1899-12-30"), format="%m/%d/%Y")
  
  #1. route/stop info
  #need to remove white space (trimws), remove N and W, need to convert coordinates so all in the same projection and datum
  ma.y.rt <- ma.y.fill[,c("SITE","STOP.#","LATITUDE","LONGITUDE")]
  ma.y.rt$LATITUDE <- trimws(ma.y.rt$LATITUDE)
  ma.y.rt$LONGITUDE <- trimws(ma.y.rt$LONGITUDE)
  
  if(y == 2013)
  {
    ma.y.rt <- separate(ma.y.rt, col="LATITUDE", into=c("lat.deg","lat.min","lat.sec","lat.dir"), remove=F, sep="°|'|\"")
    ma.y.rt <- separate(ma.y.rt, col="LONGITUDE", into=c("long.deg","long.min","long.sec","long.dir"), remove=F, sep="°|'|\"")
    ma.y.rt$x <- -1*(as.numeric(ma.y.rt$long.deg) + as.numeric(ma.y.rt$long.min)/60 + as.numeric(ma.y.rt$long.sec)/3600)
    ma.y.rt$y <- as.numeric(ma.y.rt$lat.deg) + as.numeric(ma.y.rt$lat.min)/60 + as.numeric(ma.y.rt$lat.sec)/3600
    ma.y.rt.sf <- st_as_sf(ma.y.rt[!is.na(ma.y.rt$x),], coords=c("x","y"), crs=4326)
  }
  else
  {
    ma.y.rt$LONGITUDE <- as.numeric(ma.y.rt$LONGITUDE)
    ma.y.rt$LATITUDE <- as.numeric(ma.y.rt$LATITUDE)
    
    ma.y.rt$LONGITUDE <- -1*abs(ma.y.rt$LONGITUDE)
    ma.y.rt.sf <- st_as_sf(ma.y.rt[!is.na(ma.y.rt$LATITUDE) & ma.y.rt$LATITUDE != "x",], coords=c("LONGITUDE","LATITUDE"), crs=4326)
  }
  ma.y.rt.uniform <- st_transform(ma.y.rt.sf, crs=4269) #convert to NAD83
  
  print(ggplot(data=AF_states[AF_states$ST == "MA",]) + geom_sf() + geom_sf(data=ma.y.rt.uniform) + 
          theme_bw(base_size=20)) + theme(panel.grid=element_blank())
  
  ma.y.rt <- cbind(st_drop_geometry(ma.y.rt.uniform), st_coordinates(ma.y.rt.uniform)) #get the coordinates back in CRS: 4269
  ma.y.rt <- ma.y.rt[!duplicated(ma.y.rt),]
  
  #2. survey info
  ma.y.survey <- ma.y.fill[ma.y.fill$`STOP.#` %in% c(1), c("SITE","DATE","OBSERVER","NOTES")]
  ma.y.survey$surveytime <- substr(gsub(ma.y.survey$NOTES, pattern=" ", replacement=""), 0, 9)
  ma.y.survey <- separate(ma.y.survey, col="surveytime", into=c("starttime","endtime"), sep="-", remove=T)
  
  ma.y.survey$start_hour <- paste(as.numeric(substr(ma.y.survey$starttime, 0, 2)), ":", 
                                  sprintf("%02d", as.numeric(substr(ma.y.survey$starttime, 3, 4))), sep="")
  try(ma.y.survey[grep(ma.y.survey$start_hour, pattern="NA"),]$start_hour <- NA)
  
  ma.y.survey$end_hour <- paste(as.numeric(substr(ma.y.survey$endtime, 0, 2)), ":", 
                                  sprintf("%02d", as.numeric(substr(ma.y.survey$endtime, 3, 4))), sep="")
  try(ma.y.survey[grep(ma.y.survey$end_hour, pattern="NA"),]$end_hour <- NA)
  # ma.y.survey[,c("SITE","DATE","start_hour","end_hour")]
  
  #3. obs info
  # EWPW(#) is duplicated for each stop, since recording direction and distance of each bird
  ma.y.obs <- ma.y.fill[,c("SITE","DATE","STOP.#","EWPW.(#)")]
  ma.y.obs <- ma.y.obs[!duplicated(ma.y.obs[,c("SITE","DATE","STOP.#")]),]
  ma.y.obs$`EWPW.(#)` <- as.numeric(ma.y.obs$`EWPW.(#)`)
  ma.y.obs <- ma.y.obs[!is.na(ma.y.obs$`STOP.#`),]
  
  #4. combine 1,2,3 for master ma data frame
  ma.y <- merge(ma.y.rt[,c("SITE","STOP.#","X","Y")], ma.y.obs, by=c("SITE","STOP.#"), all=T)
  ma.y <- merge(ma.y, ma.y.survey[,c("SITE","DATE","OBSERVER","NOTES","start_hour","end_hour")], by=c("SITE","DATE"), all.x=T)
  ma.y$start_year <- y
  ma <<- rbind(ma, ma.y)
  
  
  
  #combine them all separately
  ma.rt <<- rbind(ma.rt, cbind(ma.y.rt[,c("SITE","STOP.#","X","Y")], year=y))
  ma.survey <<- rbind(ma.survey, cbind(ma.y.survey[,c("SITE","DATE","OBSERVER","NOTES","start_hour","end_hour")], year=y))
  ma.obs <<- rbind(ma.obs, cbind(ma.y.obs, year=y))
}

nrow(ma.rt)
ma.rt[duplicated(ma.rt[,c("X","Y")]),]
ma.rt <- ma.rt[!duplicated(ma.rt[,c("X","Y")]),] #make sure only unique survey locations
nrow(ma.rt) #642 routes
table(ma.rt$SITE)
write.csv(ma.rt, file="../Data_raw/CombinedData/MA_nightjar_routes.csv", row.names=F)

write.csv(ma.survey, file="../Data_raw/CombinedData/MA_nightjar_surveyinfo.csv", row.names=F)
write.csv(ma.obs, file="../Data_raw/CombinedData/MA_nightjar_observations.csv", row.names=F)

write.csv(ma, file="../Data_raw/CombinedData/MA_nightjar_master_2013-2022.csv", row.names=F)

#################
#fix incorrect 
#################
non.nsn.sites[non.nsn.sites$Route.Longitude > 0 & !is.na(non.nsn.sites$Route.Longitude),]$Route.Longitude <- -(non.nsn.sites[non.nsn.sites$Route.Longitude > 0 & !is.na(non.nsn.sites$Route.Longitude),]$Route.Longitude)

table(non.nsn.sites$state)

# ontcan <- non.nsn.sites[non.nsn.sites$state == "ONTCAN",] #not in Atlantic flyway
# ma <- non.nsn.sites[non.nsn.sites$state == "MA",] 
# plot(ontcan$Route.Longitude, ontcan$Route.Latitude)
# plot(ma$Route.Longitude, ma$Route.Latitude) #no location information

unique(non.nsn.sites$state)

nrow(non.nsn.sites)

nrow(non.nsn.sites[is.na(non.nsn.sites$Route.Latitude),])
nrow(non.nsn.sites[non.nsn.sites$created_at == "",])

#some sites in the NE don't have created_at date times
# non.nsn.sites <- non.nsn.sites[!is.na(non.nsn.sites$Route.Latitude) & non.nsn.sites$created_at != "",] 
```

##Make master file of non-NSN sites and observations
NH sites: non.nsn.sites$id == non.nsn$route ish
  non.nsn.sites$id has - between state and site name
  
```{r, echo=T}
unique(non.nsn$state)

nh.sites <- non.nsn.sites[non.nsn.sites$state == "NH",]
  nh.sites$id <- gsub(nh.sites$id, pattern="-", replacement="")
nh.obs <- non.nsn[non.nsn$state == "NH",]
nh <- merge(nh.sites, nh.obs, all=T,
            by.x=c("id","Route.Stop","state"),
            by.y=c("route","stop","state"))
nh[is.na(nh$species),]$species <- "NONE"
nh[nh$species == "WHWP",]$species <- "EWPW"


ny.sites <- non.nsn.sites[non.nsn.sites$state == "NY",]
ny.missing.sites <- read.csv("../Data_raw/CombinedData/NY_MissingLocations_fromMattPalumbo.csv")
  #only need to do below once to convert NAD83 zone 18 to lat/long (copy and pasted into csv file)
  # ftdrum <- ny.missing.sites[ny.missing.sites$route == "Fort Drum",]
  # ftdrum.sf <- st_as_sf(ftdrum, coords=c("N","W"), crs=26918)
  # ftdrum.sf.latlong <- st_transform(ftdrum.sf, crs=crs(AF_states))
ny.missing.sites <- ny.missing.sites[!is.na(ny.missing.sites$Route.Latitude),]
ny.missing.sites <- ny.missing.sites[,c("route","stop","Route.Latitude","Route.Longitude")]
  names(ny.missing.sites) <- c("name","Route.Stop","Route.Latitude","Route.Longitude")
  ny.missing.sites$state <- "NY"
  
#do this once to convert from NAD83 zone18 to lat/long
# PerchRiver <- read.csv("../Data_raw/CombinedData/NY_new_PerchRiver_WPW_Route2023.csv")
#   PerchRiver$Easting <- as.numeric(gsub(PerchRiver$Easting, pattern="m", replacement=""))
#   PerchRiver$Northing <- as.numeric(gsub(PerchRiver$Northing, pattern="m", replacement=""))
#   PR.sf <- st_as_sf(PerchRiver, coords=c("Easting","Northing"), crs=26918)
#   PR.sf.latlong <- st_transform(PR.sf, crs=crs(AF_states))
  
ny.sites <- bind_rows(ny.sites, ny.missing.sites)

ny.obs <- non.nsn[non.nsn$state == "NY",]
  #need to correct route information so it matches ny.sites
  ny.correct <- read.csv("../Data_raw/combined_corrections.csv")
  ny.obs <- merge(ny.obs, ny.correct[,c("route","correct")],
                  by="route", all.x=T)
  
ny <- merge(ny.sites, ny.obs, all=T,
            by.x=c("name","Route.Stop","state"),
            by.y=c("correct","stop","state"))
ny[is.na(ny$species),]$species <- "NONE"
ny[ny$species == "-",]$species <- "NONE"
ny$species <- toupper(ny$species)
  #need to correct species codes to match other data files
  ny.sp <- read.csv("../Data_raw/combined_sp_key.csv")
  ny <- merge(ny.sp[,c("original_code","code")], ny, 
              by.y="species", by.x="original_code", all.y=T)
  names(ny) <- c("original_code","species", names(ny)[3:25])
  ny[ny$id %in% "",]$id <- ny[ny$id %in% "",]$route #original route name
  
vt.sites <- non.nsn.sites[non.nsn.sites$state == "VT",]
vt.obs <- non.nsn[non.nsn$state == "VT",] #no Vermont observations received as of 6/26/2023

vt <- merge(vt.sites, vt.obs, all=T,
            by.x=c("name","Route.Stop","state"),
            by.y=c("route","stop","state"))
vt[vt$id %in% "",]$id <- vt[vt$id %in% "",]$name

#####
# Maine
#####
#also has date and time of surveys
me.sites <- read.csv(file="../Data_raw/CombinedData/ME_NIGHTJAR Master Point Count File 2018-2022_sampling.csv") #data 2018-2022

#convert from NAD83 zone19 to lat/long
#NAD83 zone 19 EPSG:26919
me.utm.sf <- st_as_sf(me.sites, coords=c("UTM.first","UTM.second"), crs=26919)
me.latlong.sf <- st_transform(me.utm.sf, crs=crs(AF_states))
me.coords <- data.frame(st_coordinates(me.latlong.sf))
names(me.coords) <- c("Route.Longitude","Route.Latitude")
me.sites <- cbind(me.sites, me.coords)

me.sites2 <- read.csv(file="../Data_raw/CombinedData/Maine Nightjar Route Coordinates (2023) - Lat Long.csv") #contains survey locations in 2023
# me.sites2[!me.sites2$Point.Name %in% me.sites$Point.Count..,]
names(me.sites2) <- c("Point.Count..","Route.Latitude","Route.Longitude")
me.sites2 <- separate(me.sites2, col="Point.Count..", into=c("ROUTE","stop"), remove=F, sep="-")
me.sites2$YEAR <- 2023

#combine 2018-2022 and 2023 data
me.sites <- bind_rows(me.sites, me.sites2[,!names(me.sites2) %in% "stop"])

#contains data for all species heard
me.obs <- read.csv(file="../Data_raw/CombinedData/ME_NIGHTJAR Master Point Count File 2018-2022_obs.csv")
me.nightjar.obs <- me.obs[me.obs$Species %in% c("Chuck-will's-widow","Eastern Whip-poor-will","Common Nighthawk"),]
me.nightjar.obs[me.nightjar.obs$Species %in% "Chuck-will's-widow",]$Species <- "CWWI"
me.nightjar.obs[me.nightjar.obs$Species %in% "Eastern Whip-poor-will",]$Species <- "EWPW"
me.nightjar.obs[me.nightjar.obs$Species %in% "Common Nighthawk",]$Species <- "CONI"

#need to combine rows if same species, but different directions
names(me.nightjar.obs)[!names(me.nightjar.obs) %in% "Direction"]
me.nightjar.obs <- ddply(me.nightjar.obs, .(YEAR, Sampling.Event.ID, Observation.Date, Point.Count.., Sunset.Moonrise,
                                            X1st.Sunset..A...2nd.Sunset..B...1st.Moonrise..C...or.2nd.Moonrise..D.,
                                            Species), 
                         summarize, min.1=sum(Minute.1), min.2=sum(Minute.2), min.3=sum(Minute.3),
                                    min.4=sum(Minute.4), min.5=sum(Minute.5), min.6=sum(Minute.6))

#check which sites have data (all sites in me.sites were run up to 2022)
unique(me.sites[!me.sites$Point.Count.. %in%
                  unique(me.obs$Point.Count..),]$Point.Count..)
#all sites in me.sites have data associated with them (up to 2022)

#check do all obs have sites?
unique(me.obs[!me.obs$Point.Count.. %in% unique(me.sites$Point.Count..),]$Point.Count..)
#yes all obs in me.obs have site data associated with them

me <- merge(me.sites, me.nightjar.obs,
            by.x=c("Sampling.Event.UNIQUE.ID",
                   "Point.Count..","YEAR","X1st.Sunset..A...2nd.Sunset..B...1st.Moonrise..C...or.2nd.Moonrise..D."),
            by.y=c("Sampling.Event.ID","Point.Count..","YEAR",
                   "X1st.Sunset..A...2nd.Sunset..B...1st.Moonrise..C...or.2nd.Moonrise..D."), all=T)

# me[is.na(me$Species),]$Species <- "NONE" #can't do this because gives years without data a none row.
me[!is.na(me$Bird.Survey.Date..MM.DD.YYYY) & is.na(me$Species),]$Species <- "NONE"

table(me[,c("Species","YEAR")], useNA = "always")

# names(me) <- gsub(names(me), pattern="Minute", replacement="min") #don't need to do this now since combined rows above and renames to min.1
#Sampling.Event.UNIQUE.ID is uniquely identifying the survey date and time and route
#Point.Count.. uniquely identifies the route and stop
me$start_time <- paste(me$Bird.Survey.Date..MM.DD.YYYY,
                       me$Bird.Survey.start.time..in.HH.MM.24.hour.format, sep=" ")
me <- me[,c("Point.Count..","ROUTE",
            "start_time","Route.Longitude","Route.Latitude",
            "YEAR","Wind.Speed.Code","Sky.Condition.Code","Background.Noise.Code",
            "X..Cars",
        "X1st.Sunset..A...2nd.Sunset..B...1st.Moonrise..C...or.2nd.Moonrise..D.",
            "Species","min.1","min.2","min.3","min.4","min.5","min.6")]
me <- separate(me, col="Point.Count..", into=c("id","Route.Stop"),
               remove=T, sep="-")
names(me) <- c("id","Route.Stop","name","start_time",
               "Route.Longitude","Route.Latitude","year",
               "wind","sky","noise","cars","moon","species",
               "min.1","min.2","min.3","min.4","min.5","min.6")
me$state <- "ME"
me$Route.Stop <- as.numeric(me$Route.Stop)

#add in other columns in nh
me$created_at <- ""
me$updated_at <- ""
me$dir <- ""
me$stop_des <- ""

#####
# Delaware
#####
de <- read.csv("../Data_raw/CombinedData/DE_Nightjar Survey 2023.csv", header = T)
names(de) <- c("created_at","id","Route.Stop","Route.Latitude","Route.Longitude","wind","sky","noise","moon","species",
               "min.1","min.2","min.3","min.4","min.5","min.6")
de$updated_at <- ""
de$dir <- ""
de$stop_des <- ""
de$cars <- ""
de$year <- 2023
de$name <- "Milford Neck"
de$state <- "DE"
de$start_time <- "06/04/2023 21:34"

de$species <- toupper(de$species)

#
names(nh)
# "id"              "Route.Stop"      "state"           "name"
# "created_at"      "updated_at"     "Route.Latitude""Route.Longitude"
# "year"            "stop_des"        "wind"            "sky" 
# "noise"           "cars"            "moon"            "dir"           "species"
# "min.1"           "min.2"           "min.3"         "min.4"           "min.5"           "min.6"

##########
#Connecticut
##########

#match columns with ny, nh, and vt data
ct.data.all$stop <- as.numeric(substr(ct.data.all$PointID,
                           start=nchar(ct.data.all$PointID)-1, 
                           stop = nchar(ct.data.all$PointID)))
ct.data.all$species <- "NONE"
ct.data.all[ct.data.all$MaxCount > 0,]$species <- "EWPW"
ct.data.all$MaxCount <- as.numeric(ct.data.all$MaxCount)

ct <- data.frame(ct.data.all[,c("PointID","stop","RouteName",
                 "Latitude","Longitude","year","species","SumCount",
                 "MaxCount")])
names(ct) <- c("id","Route.Stop","name",
               "Route.Latitude","Route.Longitude",
               "year","species","TotalDetections","MaxDetections")
ct$state <- "CT"
ct$Route.Longitude <- -ct$Route.Longitude
ct$id <- ct$name

ct.obs.all$Route.Stop <- as.numeric(substr(ct.obs.all$PointID, start=nchar(ct.obs.all$PointID)-1, stop=nchar(ct.obs.all$PointID)))

ct <- merge(ct, ct.obs.all[,!names(ct.obs.all) %in% c("PointID","ProjectName","file")], all.x=T, by.x=c("name","Route.Stop","year"), by.y=c("RouteName","Route.Stop","year"))
names(ct) <- gsub(names(ct), pattern="Interval", replacement="min.")

ct$min.1 <- as.numeric(ct$min.1)
ct$min.2 <- as.numeric(ct$min.2)
ct$min.3 <- as.numeric(ct$min.3)
ct$min.4 <- as.numeric(ct$min.4)
ct$min.5 <- as.numeric(ct$min.5)
ct$min.6 <- as.numeric(ct$min.6)

#remove intervals 7-10 for now
ct <- ct[,!names(ct) %in% c("min.7","min.8","min.9","min.10")]

####################
#Massachusetts
####################
ma$id <- ma$SITE
ma$Route.Stop <- as.numeric(ma$`STOP.#`)
ma$state <- "MA"
ma$name <- ma$SITE
ma$Route.Latitude <- ma$Y
ma$Route.Longitude <- ma$X
ma$year <- ma$start_year
ma$min.1 <- ma$`EWPW.(#)`
ma$species <- "EWPW"
ma$TotalDetections <- ma$`EWPW.(#)`
ma$MaxDetections <- ma$`EWPW.(#)`
ma$start_time <- paste(ma$DATE, ma$start_hour, sep=" ")

#combine all non-nsn state data together
columns <- c(names(nh), "start_time")
non.nsn.master <- bind_rows(nh, ny[,names(nh)], vt[,names(nh)])
  non.nsn.master$noise <- as.integer(non.nsn.master$noise) #match classes
non.nsn.master <- bind_rows(non.nsn.master, me[,columns], de[,columns]) #these contain "start_time"

non.nsn.master$TotalDetections <- rowSums(non.nsn.master[,grep(names(non.nsn.master), pattern="min.")])
suppressWarnings(non.nsn.master$MaxDetections <- apply(non.nsn.master[,grep(names(non.nsn.master), pattern="min.")],
                                                       1, function(x) max(x, na.rm=T)))
non.nsn.master[non.nsn.master$MaxDetections == "-Inf",]$MaxDetections <- NA
non.nsn.master$Route.Stop <- as.numeric(non.nsn.master$Route.Stop)
non.nsn.master <- bind_rows(non.nsn.master, ct)
non.nsn.master <- bind_rows(non.nsn.master, ma[,names(ma) %in% names(non.nsn.master)])

#separate out month, day, and time
non.nsn.master <- separate(non.nsn.master, col=start_time, into=c("start_date","start_hour"), sep=" ", remove=F)
non.nsn.master <- separate(non.nsn.master, col=start_date, into=c("start_month","start_day","start_year"), sep="/", remove=F) #can ignore warnings

table(non.nsn.master$state)

#only run once
write.table(non.nsn.master,
            file="../Data_processed/master_nonnsn_2023-08-17.txt", sep=",", row.names=F)
```

```{r, echo=F}
##Map non NSN sites in Atlantic flyway
#not relevant anymore. only observations were in NY and NH in obs file
# #subset non.nsn.sites to only Atlantic flyway sites
# non.nsn.sites <- non.nsn.sites[non.nsn.sites$state %in%
#                                  unique(AF_states$ST),]
# 
# non.nsn.sites.sf <- st_as_sf(non.nsn.sites[!is.na(non.nsn.sites$Route.Latitude),], coords=c("Route.Longitude","Route.Latitude"), crs=projection(AF_states))
# 
# base.plot <- ggplot() + geom_sf(data=nsn.sf, col="white") + geom_sf(data=AF_states) + theme_bw() + theme(panel.grid=element_blank())
# base.plot + geom_sf(data=non.nsn.sites.sf)
# 
# af.plot <- ggplot() + geom_sf(data=AF_states, fill="white") + theme_bw() + theme(panel.grid=element_blank())
# af.plot
# af.plot + geom_sf(data=non.nsn.sites.sf)
```